{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/streetlamplee/fastapi_cj/blob/main/Openpose_%EA%B0%9C%EC%84%A0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpYuVLxB7zx8"
      },
      "source": [
        "#Mount Google Drive\n",
        "from google.colab import drive # import drive from google colab\n",
        "\n",
        "ROOT = \"/content/drive\"     # default location for the drive\n",
        "print(ROOT)                 # print content of ROOT (Optional)\n",
        "drive.mount(ROOT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2vm5gI471qi"
      },
      "source": [
        "from os.path import join\n",
        "import cv2\n",
        "\n",
        "MY_GOOGLE_DRIVE_PATH = 'C:/ehrflqtlaghkgkrtmq/' # 프로젝트 경로\n",
        "PROJECT_PATH = join(MY_GOOGLE_DRIVE_PATH) # 프로젝트 경로\n",
        "print(PROJECT_PATH)\n",
        "\n",
        "%cd \"{PROJECT_PATH}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfscJsZY74VW"
      },
      "source": [
        "!pip install opencv-python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxNNPsWqcdrZ"
      },
      "source": [
        "#openpose 데이터 증강으로 결과 확인\n",
        "\n",
        "mp4 -> jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbC66OhgPKYR"
      },
      "source": [
        "for file in file_list:\n",
        "    try:\n",
        "        if not (os.path.isdir(videoPath + file)):\n",
        "            os.makedirs(os.path.join(imagePath + file))\n",
        "\n",
        "            cap = cv2.VideoCapture(videoPath + file)\n",
        "\n",
        "            count = 0\n",
        "\n",
        "            while True:\n",
        "                ret, image = cap.read()\n",
        "\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                cv2.imwrite(imagePath + file + \"/frame%d.jpg\" % count, image)\n",
        "\n",
        "                print('%d.jpg done' % count)\n",
        "                count += 1\n",
        "\n",
        "            cap.release()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaImH1Q5Oka0"
      },
      "source": [
        "def output_keypoints(frame, net, threshold, BODY_PARTS, now_frame, total_frame):\n",
        "    global points\n",
        "\n",
        "    # 입력 이미지의 사이즈 정의\n",
        "    image_height = 368\n",
        "    image_width = 368\n",
        "\n",
        "    # 네트워크에 넣기 위한 전처리\n",
        "    input_blob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (image_width, image_height), (0, 0, 0), swapRB=False, crop=False)\n",
        "\n",
        "    # 전처리된 blob 네트워크에 입력\n",
        "    net.setInput(input_blob)\n",
        "\n",
        "    # 결과 받아오기\n",
        "    out = net.forward()\n",
        "    # The output is a 4D matrix :\n",
        "    # The first dimension being the image ID ( in case you pass more than one image to the network ).\n",
        "    # The second dimension indicates the index of a keypoint.\n",
        "    # The model produces Confidence Maps and Part Affinity maps which are all concatenated.\n",
        "    # For COCO model it consists of 57 parts – 18 keypoint confidence Maps + 1 background + 19*2 Part Affinity Maps. Similarly, for MPI, it produces 44 points.\n",
        "    # We will be using only the first few points which correspond to Keypoints.\n",
        "    # The third dimension is the height of the output map.\n",
        "    out_height = out.shape[2]\n",
        "    # The fourth dimension is the width of the output map.\n",
        "    out_width = out.shape[3]\n",
        "\n",
        "    # 원본 이미지의 높이, 너비를 받아오기\n",
        "    frame_height, frame_width = frame.shape[:2]\n",
        "\n",
        "    # 포인트 리스트 초기화\n",
        "    points = []\n",
        "\n",
        "    print(f\"============================== frame: {now_frame:.0f} / {total_frame:.0f} ==============================\")\n",
        "    for i in range(len(BODY_PARTS)):\n",
        "\n",
        "        # 신체 부위의 confidence map\n",
        "        prob_map = out[0, i, :, :]\n",
        "\n",
        "        # 최소값, 최대값, 최소값 위치, 최대값 위치\n",
        "        min_val, prob, min_loc, point = cv2.minMaxLoc(prob_map)\n",
        "\n",
        "        # 원본 이미지에 맞게 포인트 위치 조정\n",
        "        x = (frame_width * point[0]) / out_width\n",
        "        x = int(x)\n",
        "        y = (frame_height * point[1]) / out_height\n",
        "        y = int(y)\n",
        "\n",
        "        if prob > threshold:  # [pointed]\n",
        "            cv2.circle(frame, (x, y), 5, (0, 255, 255), thickness=-1, lineType=cv2.FILLED)\n",
        "            cv2.putText(frame, str(i), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 1, lineType=cv2.LINE_AA)\n",
        "\n",
        "            points.append((x, y))\n",
        "            print(f\"[pointed] {BODY_PARTS[i]} ({i}) => prob: {prob:.5f} / x: {x} / y: {y}\")\n",
        "\n",
        "        else:  # [not pointed]\n",
        "            cv2.circle(frame, (x, y), 5, (0, 255, 255), thickness=-1, lineType=cv2.FILLED)\n",
        "            cv2.putText(frame, str(i), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 1, lineType=cv2.LINE_AA)\n",
        "\n",
        "            points.append(None)\n",
        "            print(f\"[not pointed] {BODY_PARTS[i]} ({i}) => prob: {prob:.5f} / x: {x} / y: {y}\")\n",
        "\n",
        "    return frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDTCgA3yc15G"
      },
      "source": [
        "def output_keypoints_with_lines(frame, POSE_PAIRS):\n",
        "    for pair in POSE_PAIRS:\n",
        "        part_a = pair[0]  # 0 (Head)\n",
        "        part_b = pair[1]  # 1 (Neck)\n",
        "        if points[part_a] and points[part_b]:\n",
        "            print(f\"[linked] {part_a} {points[part_a]} <=> {part_b} {points[part_b]}\")\n",
        "            cv2.line(frame, points[part_a], points[part_b], (0, 255, 0), 3)\n",
        "        else:\n",
        "            print(f\"[not linked] {part_a} {points[part_a]} <=> {part_b} {points[part_b]}\")\n",
        "\n",
        "    return frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r6DQP7Pj_mO"
      },
      "source": [
        "def output_keypoints_with_lines_video(proto_file, weights_file, video_path, threshold, BODY_PARTS, POSE_PAIRS):\n",
        "\n",
        "    # 네트워크 불러오기\n",
        "    net = cv2.dnn.readNetFromCaffe(proto_file, weights_file)\n",
        "\n",
        "    # GPU 사용\n",
        "    # net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
        "    # net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
        "\n",
        "    # 비디오 읽어오기\n",
        "    capture = cv2.VideoCapture(video_path)\n",
        "    i = 0\n",
        "    while True:\n",
        "        i += 1\n",
        "        if i < 10:\n",
        "            a = '0000' + str(i)\n",
        "        elif i < 100:\n",
        "            a = '000' + str(i)\n",
        "        elif i < 1000:\n",
        "            a = '00' + str(i)\n",
        "        elif i < 10000:\n",
        "            a = '0' + str(i)\n",
        "        else:\n",
        "            a = str(i)\n",
        "\n",
        "        now_frame_boy = capture.get(cv2.CAP_PROP_POS_FRAMES)\n",
        "        total_frame_boy = capture.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "\n",
        "        if now_frame_boy == total_frame_boy:\n",
        "            break\n",
        "\n",
        "        ret, frame_boy = capture.read()\n",
        "        frame_boy = output_keypoints(frame=frame_boy, net=net, threshold=threshold, BODY_PARTS=BODY_PARTS, now_frame=now_frame_boy, total_frame=total_frame_boy)\n",
        "        frame_boy = output_keypoints_with_lines(frame=frame_boy, POSE_PAIRS=POSE_PAIRS)\n",
        "        cv2.imshow(\"Output_Keypoints\", frame_boy)\n",
        "        cv2.imwrite('C:/ehrflqtlaghkgkrtmq/openposed/frame' + a + '.jpg', frame_boy)\n",
        "\n",
        "        if cv2.waitKey(10) == 27:  # esc 입력시 종료\n",
        "            break\n",
        "\n",
        "    capture.release()\n",
        "    cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFtYF1Omkg_L"
      },
      "source": [
        "Openpose 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQd6_uTEF0IK"
      },
      "source": [
        "BODY_PARTS_MPI = {0: \"Head\", 1: \"Neck\", 2: \"RShoulder\", 3: \"RElbow\", 4: \"RWrist\",\n",
        "                  5: \"LShoulder\", 6: \"LElbow\", 7: \"LWrist\", 8: \"RHip\", 9: \"RKnee\",\n",
        "                  10: \"RAnkle\", 11: \"LHip\", 12: \"LKnee\", 13: \"LAnkle\", 14: \"Chest\",\n",
        "                  15: \"Background\"}\n",
        "\n",
        "POSE_PAIRS_MPI = [[0, 1], [1, 2], [1, 5], [1, 14], [2, 3], [3, 4], [5, 6],\n",
        "                  [6, 7], [8, 9], [9, 10], [11, 12], [12, 13], [14, 8], [14, 11]]\n",
        "\n",
        "BODY_PARTS_COCO = {0: \"Nose\", 1: \"Neck\", 2: \"RShoulder\", 3: \"RElbow\", 4: \"RWrist\",\n",
        "                   5: \"LShoulder\", 6: \"LElbow\", 7: \"LWrist\", 8: \"RHip\", 9: \"RKnee\",\n",
        "                   10: \"RAnkle\", 11: \"LHip\", 12: \"LKnee\", 13: \"LAnkle\", 14: \"REye\",\n",
        "                   15: \"LEye\", 16: \"REar\", 17: \"LEar\", 18: \"Background\"}\n",
        "\n",
        "POSE_PAIRS_COCO = [[0, 1], [0, 14], [0, 15], [1, 2], [1, 5], [1, 8], [1, 11], [2, 3], [3, 4],\n",
        "                   [5, 6], [6, 7], [8, 9], [9, 10], [12, 13], [11, 12], [14, 16], [15, 17]]\n",
        "\n",
        "BODY_PARTS_BODY_25 = {0: \"Nose\", 1: \"Neck\", 2: \"RShoulder\", 3: \"RElbow\", 4: \"RWrist\",\n",
        "                      5: \"LShoulder\", 6: \"LElbow\", 7: \"LWrist\", 8: \"MidHip\", 9: \"RHip\",\n",
        "                      10: \"RKnee\", 11: \"RAnkle\", 12: \"LHip\", 13: \"LKnee\", 14: \"LAnkle\",\n",
        "                      15: \"REye\", 16: \"LEye\", 17: \"REar\", 18: \"LEar\", 19: \"LBigToe\",\n",
        "                      20: \"LSmallToe\", 21: \"LHeel\", 22: \"RBigToe\", 23: \"RSmallToe\", 24: \"RHeel\", 25: \"Background\"}\n",
        "\n",
        "POSE_PAIRS_BODY_25 = [[0, 1], [0, 15], [0, 16], [1, 2], [1, 5], [1, 8], [8, 9], [8, 12], [9, 10], [12, 13], [2, 3],\n",
        "                      [3, 4], [5, 6], [6, 7], [10, 11], [13, 14], [15, 17], [16, 18], [14, 21], [19, 21], [20, 21],\n",
        "                      [11, 24], [22, 24], [23, 24]]\n",
        "\n",
        "# 신경 네트워크의 구조를 지정하는 prototxt 파일 (다양한 계층이 배열되는 방법 등) // <-- 컴퓨터 & 설정에 맞게 바꿀부분\n",
        "protoFile_mpi = \"C:\\\\openpose\\\\models\\\\pose\\\\mpi\\\\pose_deploy_linevec.prototxt\"\n",
        "protoFile_mpi_faster = \"C:\\\\openpose\\\\models\\\\pose\\\\mpi\\\\pose_deploy_linevec_faster_4_stages.prototxt\"\n",
        "protoFile_coco = \"C:\\\\openpose\\\\models\\\\pose\\\\coco\\\\pose_deploy_linevec.prototxt\"\n",
        "protoFile_body_25 = \"C:\\\\openpose\\\\models\\\\pose\\\\body_25\\\\pose_deploy.prototxt\"\n",
        "\n",
        "# 훈련된 모델의 weight 를 저장하는 caffemodel 파일\n",
        "weightsFile_mpi = \"C:\\\\openpose\\\\models\\\\pose\\\\mpi\\\\pose_iter_160000.caffemodel\"\n",
        "weightsFile_coco = \"C:\\\\openpose\\\\models\\\\pose\\\\coco\\\\pose_iter_440000.caffemodel\"\n",
        "weightsFile_body_25 = \"C:\\\\openpose\\\\models\\\\pose\\\\body_25\\\\pose_iter_584000.caffemodel\"\n",
        "\n",
        "# 비디오 경로\n",
        "man = \"C:\\\\ehrflqtlaghkgkrtmq/pre-openposed/20211104_190238.mp4\"  # (출처: https://pixabay.com/videos/id-25553/)\n",
        "\n",
        "# 키포인트를 저장할 빈 리스트\n",
        "points = []\n",
        "\n",
        "output_keypoints_with_lines_video(proto_file=protoFile_mpi_faster, weights_file=weightsFile_mpi, video_path=man,\n",
        "                                  threshold=0.1, BODY_PARTS=BODY_PARTS_MPI, POSE_PAIRS=POSE_PAIRS_MPI)\n",
        "\n",
        "# output_keypoints_with_lines_video(proto_file=protoFile_coco, weights_file=weightsFile_coco, video_path=man,\n",
        "#                                   threshold=0.1, BODY_PARTS=BODY_PARTS_COCO, POSE_PAIRS=POSE_PAIRS_COCO)\n",
        "\n",
        "# output_keypoints_with_lines_video(proto_file=protoFile_body_25, weights_file=weightsFile_body_25, video_path=man,\n",
        "#                                   threshold=0.1, BODY_PARTS=BODY_PARTS_BODY_25, POSE_PAIRS=POSE_PAIRS_BODY_25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToRflY05ncpT"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTqU2gyUcr3l"
      },
      "source": [
        "jpg -> mp4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEZCxQ4KRnMa"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "%cd \"{PROJECT_PATH}\"\n",
        "file_name = '20211104_190238.mp4'\n",
        "path = 'openposed/'\n",
        "paths = [os.path.join(path , i ) for i in os.listdir(path) if re.search(\".jpg$\", i )]\n",
        "## 정렬 작업\n",
        "\n",
        "\n",
        "#paths = list(np.sort(store1))\n",
        "#len('ims/2/a/2a.2710.png')\n",
        "# print(paths)\n",
        "\n",
        "pathIn = './openposed/'\n",
        "pathOut = './Video/result_of_' + str(file_name) + '.mp4'\n",
        "fps = 5\n",
        "import cv2\n",
        "frame_array = []\n",
        "for y in range(len(paths)):\n",
        "    img = cv2.imread(str(PROJECT_PATH) + str(paths[y]))\n",
        "    #img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
        "#     print('image/' + str(foldername[x]) + '/' + str(framename[x]) + str(y) + '.jpg')\n",
        "    height, width, layers = img.shape\n",
        "    size = (width,height)\n",
        "    frame_array.append(img)\n",
        "out = cv2.VideoWriter(pathOut,cv2.VideoWriter_fourcc(*'DIVX'), fps, size)\n",
        "for i in range(len(frame_array)):\n",
        "    # writing to a image array\n",
        "    out.write(frame_array[i])\n",
        "out.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtKW-sexOE35"
      },
      "source": [
        "paths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0SbkUM67BxX"
      },
      "source": [
        "# Pillow\n",
        "#### 그림파일의 기초적인 조정 가능\n",
        "* 보기(디버깅용)(코랩에선 불가능 so IPython.display 사용)\n",
        "* 저장 (save)\n",
        "* 자르기 (crop)\n",
        "* 사이즈 조정 (resize)\n",
        "* 회전 (rotate)\n",
        "* 향상 (대비, 밝기, 색상 밸런스, 선명도 변경) (enhance\n",
        "* etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iR6tdqIh9Isf"
      },
      "source": [
        "# from PIL import Image as img\n",
        "# from IPython.display import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-7y8f5c95s9"
      },
      "source": [
        "# img1 = img.open('test.JPG')\n",
        "# display(img1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH0fe4bER-lR"
      },
      "source": [
        "# grayimage = img1.convert(\"L\")\n",
        "# display(grayimage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL7Jz2fq-CyR"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}